from abc import ABC, abstractmethod

from ntm_cnn_ddpg.ddpg.ddpg import OptionalSeqTensors


# TODO внедрить использование класса в классы Buffer и DDPG
class State(ABC):
    """
    Абстрактнвый класс для хранения состояния среды в Q-learning алгоритмах, использующих буфер примеров
    состояния среды, действия, вознаграждения и нового состояния среды.

    Состояние среды может иметь сложную структуру, которую невыгодно постоянно хранить в виде числовой матрицы,
    ожидаемой на входе actor или critic модели.

    Если исходная среда является немарковским процессом, то для приближения к марковскому процесссу может понадобится
    перейти к производной среде, состояние которой представлено историей состояний исходной среды, действий
    и вознаграждений. При этом соседние по времени состояния производной среды значительно пересекаются по включенным
    в историю (с разным сдвигом по времени) состояний исходной среды. Для экономии памяти выгодно хранить историю как
    список ссылок на состояния исходной среды. Непосредственно перед подачей истории на вход модели
    (например, модели типа LSTM), историю необходимо преобразовать в числовую матрицу.
    """

    @abstractmethod
    def to_actor_model_input_tensor(self) -> OptionalSeqTensors:
        """
        Приведение состояния среды в формат набора тензоров,
        подходящий для подачи на вход actor модели без сложных преобразований.
        Измерение пакета не добавляется.
        Для вычисления critic модели результат необходимо предварительно объеденить с тензором действия.

        :return: Набор тензоров, подходящий для подачи на вход actor модели.
        """
        pass
